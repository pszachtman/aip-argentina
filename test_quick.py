#!/usr/bin/env python3
"""
Prueba r√°pida del scraper con l√≠mites de seguridad
"""

import asyncio
import sys
from pathlib import Path

# Importar solo lo necesario
from aip_scraper import AIPScraper

async def test_quick_scraping():
    """Prueba r√°pida que extrae solo algunos documentos"""
    print("=== Prueba r√°pida de scraping (con l√≠mites) ===")
    
    try:
        scraper = AIPScraper()
        
        from playwright.async_api import async_playwright
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                await page.goto("https://ais.anac.gob.ar/aip")
                await page.wait_for_load_state('networkidle')
                
                print("‚úì Sitio web cargado correctamente")
                
                # Probar solo secci√≥n GEN con l√≠mites estrictos
                print("\nProbando extracci√≥n de secci√≥n GEN (limitada)...")
                gen_docs = await scraper.scrape_section_documents(page, 'GEN')
                
                print(f"\n‚úÖ √âxito: Encontrados {len(gen_docs)} documentos √∫nicos en GEN")
                
                # Mostrar algunos ejemplos
                print("\nüìÑ Primeros 10 documentos encontrados:")
                for i, doc in enumerate(gen_docs[:10]):
                    print(f"  {i+1:2d}. {doc.title}")
                    print(f"      üìÖ {doc.version}")
                
                # Verificar filtrado de AD si hay tiempo
                print("\nüîç Probando filtrado de documentos AD...")
                test_titles = [
                    "AD-0.6 Indices - Indice",
                    "AD-1.1 AD/HEL Introducci√≥n",
                    "SADF-AD-2.0 Aer√≥dromos - Datos del AD SAN FERNANDO",
                    "SABE-AD-2.0 Aer√≥dromos - Datos del AD BUENOS AIRES"
                ]
                
                print("Resultados del filtrado:")
                for title in test_titles:
                    should_include = scraper._should_include_document(title, 'AD')
                    status = "‚úÖ Incluido" if should_include else "‚ùå Excluido"
                    print(f"  {status}: {title}")
                
                print("\nüéâ ¬°Prueba r√°pida completada exitosamente!")
                print(f"üìä El scraper funciona correctamente y encontr√≥ {len(gen_docs)} documentos")
                
                return True
                
            finally:
                await browser.close()
                
    except Exception as e:
        print(f"‚ùå Error en prueba r√°pida: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """Funci√≥n principal de prueba r√°pida"""
    print("AIP Argentina - Prueba R√°pida\n")
    
    success = await test_quick_scraping()
    
    if success:
        print(f"\n{'='*60}")
        print("‚úÖ PRUEBA EXITOSA - El scraper funciona correctamente")
        print('='*60)
        print("\nüöÄ Pr√≥ximos pasos:")
        print("1. Ejecutar scraper completo: python aip_scraper.py")
        print("2. O ejecutar por secciones para mayor control")
        print("\nüí° El problema del bucle infinito ha sido corregido")
        return 0
    else:
        print(f"\n{'='*60}")
        print("‚ùå PRUEBA FALL√ì - Revisar configuraci√≥n")
        print('='*60)
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
